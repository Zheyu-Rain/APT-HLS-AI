task                       : regression
subtask                    : dse
plot_dse                   : False
vis_per_kernel             : True
force_regen                : False
min_allowed_latency        : 100.0
epsilon                    : 0.001
normalizer                 : 10000000.0
util_normalizer            : 1
max_number                 : 10000000000.0
norm_method                : speedup-log2
new_speedup                : True
invalid                    : False
encode_log                 : False
v_db                       : v21
test_kernels               : None
target_kernel              : gemm-blocked
all_kernels                : False
dataset                    : harp
benchmarks                 : ['machsuite',
                           :  'poly']
tag                        : whole-machsuite-poly
graph_type                 : extended-pseudo-block-connected-hierarchy
encoder_path               : /home/zheyu/HARP/models/encoders/pragma_as_MLP-encoders.klepto
encode_edge                : True
encode_edge_position       : True
num_layers                 : 6
num_features               : 153
edge_dim                   : 335
target                     : ['perf',
                           :  'util-LUT',
                           :  'util-FF',
                           :  'util-DSP',
                           :  'util-BRAM']
MLP_common_lyr             : 0
gnn_type                   : transformer
dropout                    : 0.1
jkn_mode                   : max
jkn_enable                 : True
node_attention             : True
node_attention_MLP         : False
separate_P                 : True
separate_icmp              : False
separate_T                 : False
separate_pseudo            : True
P_use_all_nodes            : True
gae_T                      : False
gae_P                      : False
gnn_layer_after_MLP        : 1
pragma_as_MLP              : True
pragma_as_MLP_list         : ['tile',
                           :  'pipeline',
                           :  'parallel']
pragma_scope               : block
keep_pragma_attribute      : False
pragma_order               : parallel_and_merge
pragma_MLP_hidden_channels : [in_D // 2]
merge_MLP_hidden_channels  : [in_D // 2]
model_path                 : ['/home/zheyu/HARP/models/v21/regression_pragma_as_MLP_2l-m-2l-p_model_state_dict.pth']
ensemble                   : 0
ensemble_weights           : None
class_model_path           : /home/zheyu/HARP/models/v21/class_pragma_as_MLP_2l-m-2l-p_model_state_dict.pth
feature_extract            : False
fix_gnn_layer              : 1
FT_extra                   : False
save_model                 : True
resample                   : False
val_ratio                  : 0.15
activation                 : elu
D                          : 64
lr                         : 0.001
weight_decay               : 0
scheduler                  : cosine
warmup                     : linear
random_seed                : 123
batch_size                 : 64
loss                       : MSE
epoch_num                  : 200
device                     : cuda:0
explorer                   : exhaustive
model_tag                  : test
prune_util                 : True
prune_class                : True
print_every_iter           : 100
plot_pred_points           : True
user                       : zheyu
hostname                   : storm
ts                         : 2024-09-12T11-41-02.161616


##################################
Net(
  (conv_first): TransformerConv(153, 64, heads=1)
  (conv_layers): ModuleList(
    (0): TransformerConv(64, 64, heads=1)
    (1): TransformerConv(64, 64, heads=1)
    (2): TransformerConv(64, 64, heads=1)
    (3): TransformerConv(64, 64, heads=1)
    (4): TransformerConv(64, 64, heads=1)
    (5): TransformerConv(64, 64, heads=1)
  )
  (jkn): JumpingKnowledge(max)
  (loss_function): MSELoss()
  (gate_nn_P): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=1, bias=True)
  )
  (glob_P): MyGlobalAttention(gate_nn=Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=1, bias=True)
  ), nn=None)
  (gate_nn_pseudo_B): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=1, bias=True)
  )
  (glob_pseudo_B): MyGlobalAttention(gate_nn=Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=1, bias=True)
  ), nn=None)
  (MLPs): MLP_multi_objective(
    (activation): ELU(alpha=1.0)
    (layers_common): ModuleList()
    (MLP_heads): ModuleDict(
      (perf): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
      (util-LUT): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
      (util-FF): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
      (util-DSP): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
      (util-BRAM): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
    )
  )
  (MLPs_per_pragma): ModuleDict(
    (tile): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=65, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (pipeline): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=65, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (parallel): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=66, out_features=33, bias=True)
        (1): Linear(in_features=33, out_features=64, bias=True)
      )
    )
    (merge): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): Linear(in_features=96, out_features=64, bias=True)
      )
    )
  )
)
